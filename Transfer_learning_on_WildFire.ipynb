{"cells":[{"cell_type":"markdown","metadata":{"id":"Jd9lxKqjPJVJ"},"source":["**Objective :** This project aims to develop a deep learning model to identify whether an image contains a forest fire using a convolutional neural network (CNN). The model is trained using a dataset of forest fire images and evaluated for its accuracy and loss.\n","\n","**Data Preprocessing and Augmentation :**\n","\n","The project begins by importing necessary libraries, including TensorFlow and Keras for model creation and data manipulation. The dataset for training and testing the model is stored in Google Drive.\n","\n","The ImageDataGenerator class from Keras is used to perform data augmentation on the training set. This includes:\n","\n","*   Rescaling the pixel values to the range [0, 1].\n","\n","*   Shear transformations and zoom to increase variability.\n","\n","*   Horizontal flipping for further augmentation.\n","\n","\n","\n","**Model Architecture :**\n","\n","The model uses a VGG16 pre-trained model, which is loaded without its top classification layer. The base of VGG16 is frozen to retain the learned features, and new fully connected layers are added for the classification task. The network structure is as follows:\n","\n","**Base Model:** VGG16 (with include_top=False) to extract feature representations from images.\n","\n","**Additional Layers:**\n","MaxPooling2D layer for downsampling.\n","\n","Flatten to transform the 2D feature maps into 1D vectors.\n","\n","Dense layers with increasing complexity: 1520 units, 750 units, 64 units, 32 units.\n","\n","A Dropout layer (rate=0.5) to prevent overfitting.\n","\n","The output layer consists of a single neuron with a sigmoid activation function for binary classification.\n","\n","\n","**Model Compilation and Training :**\n","\n","The model is compiled with the binary crossentropy loss function and Adam optimizer with a learning rate of 0.0001. The model is trained for 10 epochs using the training generator for batches of 16 images at a time. The training process is monitored by evaluating both training accuracy and validation accuracy.\n","\n","\n","**Evaluation and Results :**\n","\n","The model's performance is evaluated on the validation set, yielding the following results:\n","\n","Test Loss: 0.2519\n","\n","Test Accuracy: 93.95%\n","\n","This indicates that the model is performing well with a high level of accuracy.\n","\n","\n","**Visualization :**\n","\n","To understand the model's learning over time, two plots are generated to visualize:\n","\n","**Training vs Validation Accuracy –** Displays how accuracy improves for both training and validation sets.\n","\n","**Training vs Validation Loss –** Shows the loss trend for both sets, which is essential for detecting overfitting or underfitting.\n","\n","\n","**Model Deployment using Gradio:**\n","\n","To make the model more accessible, a Gradio interface is created, allowing users to upload an image and receive a prediction (Fire or Non-Fire). The image is preprocessed (resized, normalized) before being passed to the model for classification. The output is a label indicating whether a forest fire is detected.\n","\n","\n","**Conclusion :**\n","\n","The model successfully detects forest fires with an accuracy of 93.95% after 10 epochs. By leveraging transfer learning with the VGG16 pre-trained model and applying appropriate data augmentation and regularization techniques, the model is able to generalize well. The Gradio interface makes it easy for users to interact with the model and make predictions on new images.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":14954,"status":"ok","timestamp":1732884857136,"user":{"displayName":"Preet Sharma","userId":"14358399268192137659"},"user_tz":-330},"id":"NtjJdfBP5kWU","outputId":"0548843d-da23-4d95-ee5c-d44c20144651"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gradio\n","  Downloading gradio-5.7.1-py3-none-any.whl.metadata (16 kB)\n","Collecting aiofiles\u003c24.0,\u003e=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio\u003c5.0,\u003e=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi\u003c1.0,\u003e=0.115.2 (from gradio)\n","  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.5.0 (from gradio)\n","  Downloading gradio_client-1.5.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: httpx\u003e=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n","Requirement already satisfied: huggingface-hub\u003e=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n","Requirement already satisfied: jinja2\u003c4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Collecting markupsafe~=2.0 (from gradio)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy\u003c3.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n","Requirement already satisfied: pandas\u003c3.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow\u003c12.0,\u003e=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n","Requirement already satisfied: pydantic\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart==0.0.12 (from gradio)\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: pyyaml\u003c7.0,\u003e=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff\u003e=0.2.2 (from gradio)\n","  Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting safehttpx\u003c1.0,\u003e=0.1.1 (from gradio)\n","  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette\u003c1.0,\u003e=0.40.0 (from gradio)\n","  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer\u003c1.0,\u003e=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Collecting uvicorn\u003e=0.14.0 (from gradio)\n","  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.0-\u003egradio) (2024.10.0)\n","Collecting websockets\u003c13.0,\u003e=10.0 (from gradio-client==1.5.0-\u003egradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna\u003e=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5.0,\u003e=3.0-\u003egradio) (3.10)\n","Requirement already satisfied: sniffio\u003e=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5.0,\u003e=3.0-\u003egradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5.0,\u003e=3.0-\u003egradio) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.24.1-\u003egradio) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.24.1-\u003egradio) (1.0.7)\n","Requirement already satisfied: h11\u003c0.15,\u003e=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*-\u003ehttpx\u003e=0.24.1-\u003egradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.25.1-\u003egradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.25.1-\u003egradio) (2.32.3)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.25.1-\u003egradio) (4.66.6)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas\u003c3.0,\u003e=1.0-\u003egradio) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas\u003c3.0,\u003e=1.0-\u003egradio) (2024.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas\u003c3.0,\u003e=1.0-\u003egradio) (2024.2)\n","Requirement already satisfied: annotated-types\u003e=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003e=2.0-\u003egradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003e=2.0-\u003egradio) (2.23.4)\n","Requirement already satisfied: click\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0,\u003e=0.12-\u003egradio) (8.1.7)\n","Requirement already satisfied: shellingham\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0,\u003e=0.12-\u003egradio) (1.5.4)\n","Requirement already satisfied: rich\u003e=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0,\u003e=0.12-\u003egradio) (13.9.4)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas\u003c3.0,\u003e=1.0-\u003egradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003c1.0,\u003e=0.12-\u003egradio) (3.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003c1.0,\u003e=0.12-\u003egradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.25.1-\u003egradio) (3.4.0)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.25.1-\u003egradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich\u003e=10.11.0-\u003etyper\u003c1.0,\u003e=0.12-\u003egradio) (0.1.2)\n","Downloading gradio-5.7.1-py3-none-any.whl (57.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.5.0-py3-none-any.whl (320 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","Successfully installed aiofiles-23.2.1 fastapi-0.115.5 ffmpy-0.4.0 gradio-5.7.1 gradio-client-1.5.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.8.1 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.12.0 uvicorn-0.32.1 websockets-12.0\n"]}],"source":["pip install gradio\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPCQWvQ6rN4P"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential,load_model\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import VGG16\n","import numpy as np\n","import gradio as gr\n","from tensorflow.keras.preprocessing import image\n","import tensorflow as tf\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22214,"status":"ok","timestamp":1732884897017,"user":{"displayName":"Preet Sharma","userId":"14358399268192137659"},"user_tz":-330},"id":"q-RfdN1i1b8t","outputId":"268a2ed0-5829-4b65-8c76-746afa269a54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3385,"status":"ok","timestamp":1732884900399,"user":{"displayName":"Preet Sharma","userId":"14358399268192137659"},"user_tz":-330},"id":"Ah9Zu8aClE_X","outputId":"9993b7c0-2e01-4da1-8f9c-ebb53b848d7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1520 images belonging to 2 classes.\n","Found 380 images belonging to 2 classes.\n","Class indices (label mapping): {'fire': 0, 'nofire': 1}\n"]}],"source":["# Set the train and validation data directories\n","train_data_dir = '/content/drive/My Drive/Deep Learning/forest_fire/Training and Validation'\n","validation_data_dir = '/content/drive/My Drive/Deep Learning/forest_fire/Testing'\n","\n","# Image dimensions\n","img_width, img_height = 224, 224\n","batch_size = 16\n","\n","# Data augmentation and preprocessing\n","train_datagen = ImageDataGenerator(\n","    rescale=1.0/255,  # Normalize pixels\n","    shear_range=0.2,  # Apply random shear transformations\n","    zoom_range=0.2,   # Apply random zoom\n","    horizontal_flip=True  # Flip images horizontally\n",")\n","\n","test_datagen = ImageDataGenerator(rescale=1.0/255)\n","\n","# Load training data and augment it\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=16,\n","    class_mode='binary'  # 'binary' for 2-class classification (fire and no-fire)\n",")\n","\n","# Load validation data\n","validation_generator = test_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=16,\n","    class_mode='binary'\n",")\n","\n","# Verify class labels\n","print(\"Class indices (label mapping):\", train_generator.class_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1125,"status":"ok","timestamp":1732884901521,"user":{"displayName":"Preet Sharma","userId":"14358399268192137659"},"user_tz":-330},"id":"KArao0bUy8DP","outputId":"f0e92468-7a62-4639-d9e1-ca72b7f4635d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}],"source":["# Load the pre-trained VGG16 model (without the top classification layer)\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","\n","# Freeze the base model layers\n","base_model.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1732884901916,"user":{"displayName":"Preet Sharma","userId":"14358399268192137659"},"user_tz":-330},"id":"NW2yHn5io5vc","outputId":"21ea909d-9b80-48eb-f54c-e00f4fba466f"},"outputs":[{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003eModel: \"sequential\"\u003c/span\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u003cspan style=\"font-weight: bold\"\u003e Layer (type)                         \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e Output Shape                \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e         Param # \u003c/span\u003e┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ vgg16 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eFunctional\u003c/span\u003e)                   │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e7\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e7\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e512\u003c/span\u003e)           │      \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e14,714,688\u003c/span\u003e │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eMaxPooling2D\u003c/span\u003e)         │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e3\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e3\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e512\u003c/span\u003e)           │               \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eFlatten\u003c/span\u003e)                    │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e4608\u003c/span\u003e)                │               \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                        │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e1520\u003c/span\u003e)                │       \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e7,005,680\u003c/span\u003e │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                      │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e750\u003c/span\u003e)                 │       \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e1,140,750\u003c/span\u003e │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                      │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e)                  │          \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e48,064\u003c/span\u003e │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_3 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                      │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32\u003c/span\u003e)                  │           \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e2,080\u003c/span\u003e │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDropout\u003c/span\u003e)                    │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e32\u003c/span\u003e)                  │               \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_4 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                      │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e1\u003c/span\u003e)                   │              \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e33\u003c/span\u003e │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","\u003c/pre\u003e\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1520\u001b[0m)                │       \u001b[38;5;34m7,005,680\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m750\u001b[0m)                 │       \u001b[38;5;34m1,140,750\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m48,064\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Total params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e22,911,295\u003c/span\u003e (87.40 MB)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,911,295\u001b[0m (87.40 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e8,196,607\u003c/span\u003e (31.27 MB)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,196,607\u001b[0m (31.27 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Non-trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e14,714,688\u003c/span\u003e (56.13 MB)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["# transfer model\n","model = Sequential()\n","\n","model.add(base_model)\n","model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n","\n","# Flatten the output of the layers\n","model.add(Flatten())\n","\n","# Fully connected layers\n","model.add(Dense(1520, activation='relu'))\n","model.add(Dense(750, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(32, activation='relu'))\n","\n","# Dropout layer for regularization\n","model.add(Dropout(0.5))\n","\n","# Output layer with one neuron (binary classification)\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy',\n","              optimizer=Adam(learning_rate=0.0001),\n","              metrics=['accuracy'])\n","\n","# Summary of the model\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"QaNd2wEao9vl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1256s\u001b[0m 13s/step - accuracy: 0.7945 - loss: 0.3975 - val_accuracy: 0.8816 - val_loss: 0.3422\n","Epoch 2/10\n","\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1207s\u001b[0m 13s/step - accuracy: 0.9473 - loss: 0.1669 - val_accuracy: 0.9000 - val_loss: 0.2311\n","Epoch 3/10\n","\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1220s\u001b[0m 13s/step - accuracy: 0.9645 - loss: 0.1059 - val_accuracy: 0.9421 - val_loss: 0.1730\n","Epoch 4/10\n","\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1222s\u001b[0m 13s/step - accuracy: 0.9633 - loss: 0.1041 - val_accuracy: 0.9211 - val_loss: 0.2259\n","Epoch 5/10\n","\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1208s\u001b[0m 13s/step - accuracy: 0.9754 - loss: 0.0928 - val_accuracy: 0.9421 - val_loss: 0.1540\n","Epoch 6/10\n","\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.9771 - loss: 0.0886 "]}],"source":["# Train the model\n","history = model.fit(\n","    train_generator,\n","    epochs=10,\n","    validation_data=validation_generator\n",")\n","\n","# Evaluate the model\n","score = model.evaluate(validation_generator, verbose=0)\n","print(f'Test loss: {score[0]}')\n","print(f'Test accuracy: {score[1]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPLOLv8X_8pp"},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4Nhe4W-OjAW"},"outputs":[],"source":["fig, axes = plt.subplots(1, 2, figsize=(10, 6))  # 1 row, 2 columns\n","\n","# Plotting accuracy and validation accuracy\n","axes[0].plot(history.history['accuracy'], label='Training accuracy')\n","axes[0].plot(history.history['val_accuracy'], label='Validation accuracy')\n","axes[0].set_title('Training and Validation Accuracy')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].legend()\n","\n","# Plotting loss and validation loss\n","axes[1].plot(history.history['loss'], label='Training loss')\n","axes[1].plot(history.history['val_loss'], label='Validation loss')\n","axes[1].set_title('Training and Validation Loss')\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Loss')\n","axes[1].legend()\n","\n","# Adjust layout and display\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmKqNw17v1tY"},"outputs":[],"source":["model.save(\"/content/drive/My Drive/Deep Learning/wildfire_model.keras\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pv33GRVXvJ3O"},"outputs":[],"source":["# Load your trained model (replace with the correct model path)\n","model = load_model(r\"/content/drive/My Drive/Deep Learning/wildfire_model.keras\")\n","\n","# Function to preprocess the image\n","def preprocess_image(image_path):\n","    img = image.load_img(image_path, target_size=(224, 224))  # Resize the image to the model's expected input size\n","    img_array = image.img_to_array(img)  # Convert the image to a numpy array\n","    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n","    img_array /= 255.  # Normalize pixel values to [0, 1]\n","    return img_array\n","\n","# Function to make the prediction\n","def predict_fire(image):\n","    img_array = preprocess_image(image)  # Preprocess the input image\n","    prediction = model.predict(img_array)  # Make prediction using the model\n","\n","    # Interpret the prediction (assuming binary classification)\n","    if prediction[0][0] \u003e 0.5:\n","        return \"Non-Fire\"\n","    else:\n","        return \"Fire\"\n","\n","# Create Gradio interface\n","interface = gr.Interface(\n","    fn=predict_fire,\n","    inputs=gr.Image(type=\"filepath\"),\n","    outputs=\"text\",\n","    title=\"Forest Fire Prediction\",\n","    description=\"Upload an image to predict if it contains a forest fire or not.\"\n",")\n","\n","\n","# Launch the Gradio interface\n","interface.launch()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dp1J7HyKOVSM"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMO/BXiO9Gv98+VuDAyrdMh","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}